{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_convolutions_my_try.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "4embtkV0pNxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 4\n",
        "------------\n",
        "\n",
        "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
        "\n",
        "The goal of this assignment is make the neural network convolutional."
      ]
    },
    {
      "metadata": {
        "id": "tm2CQN_Cpwj0",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxBQjkHEpDzU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check whether GPU is available"
      ]
    },
    {
      "metadata": {
        "id": "KwnGsyqapDzV",
        "colab_type": "code",
        "outputId": "4c001f42-dd63-4535-90d9-821063d9ea82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6548502576229029977, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11288962663\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 16407767318875256520\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "y3-cj1bpmuxc",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "ccf16ebe-7e0e-44a9-daf8-e48f26ff8614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "pickle_file = 'notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print('Training set', train_dataset.shape, train_labels.shape)\n",
        "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "  print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L7aHrm6nGDMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reformat into a TensorFlow-friendly shape:\n",
        "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
        "- labels as float 1-hot encodings."
      ]
    },
    {
      "metadata": {
        "id": "IRSyYiIIGIzS",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "2ef2c78a-11fc-433a-a99f-e334cc9f27e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "num_channels = 1 # grayscale\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape(\n",
        "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28, 1) (200000, 10)\n",
            "Validation set (10000, 28, 28, 1) (10000, 10)\n",
            "Test set (10000, 28, 28, 1) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zx2XNpM4pDzf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size = 28\n",
        "num_classes = 10\n",
        "train_size  = 200000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AgQDIREv02p1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rhgjmROXu2O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
      ]
    },
    {
      "metadata": {
        "id": "XtXOArBipDzk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# implementation 1 using method tf.nn.conv2d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEBkf7pJpDzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Important points to remember\n",
        "\n",
        "In tensorflow \"valid\" padding means we never pad and only apply the filter. while \"same\" padding means we pad equally on all sides such that   \n",
        "\n",
        "output_size = [input/stride] [] for GIF\n"
      ]
    },
    {
      "metadata": {
        "id": "lSHlNColpDzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Things to remember\n",
        "\n",
        "For random initialization of weights you can try stddev = sqrt(1/n) for normal layers and sqrt(2/n) for relu layers.\n",
        "\n",
        "I observed great changes in results when I switched weight initializations from stddev of 1 to 0.1 initially and then to above mentioned values"
      ]
    },
    {
      "metadata": {
        "id": "bxScpkr5pDzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 4096\n",
        "kernel_size_1 = 4\n",
        "kernel_size = 4\n",
        "num_channels = 1\n",
        "depth_1 = 16\n",
        "depth_2 = 32\n",
        "fc_2_nodes = 512\n",
        "fc_3_nodes = 512\n",
        "\n",
        "\n",
        "\n",
        "graph_conv1 = tf.Graph()\n",
        "\n",
        "with graph_conv1.as_default():\n",
        "    \n",
        "    #input_data_placeholder\n",
        "    X_train = tf.placeholder(tf.float32, shape = [batch_size,img_size,img_size,num_channels])\n",
        "    y_train = tf.placeholder(tf.float32, shape = [batch_size,num_classes])\n",
        "    \n",
        "    X_valid = tf.constant(valid_dataset)\n",
        "    X_test  = tf.constant(test_dataset)\n",
        "    \n",
        "    #initial_layers\n",
        "    conv_1_weights = (tf.Variable(tf.truncated_normal\n",
        "                          ([kernel_size,kernel_size,num_channels,depth_1], stddev = tf.sqrt(1/batch_size))))\n",
        "    conv_1_biases  = tf.Variable(tf.zeros(depth_1))\n",
        "    \n",
        "    #pool1 will be here\n",
        "    \n",
        "    conv_2_weights = (tf.Variable(tf.truncated_normal\n",
        "                                 ([kernel_size,kernel_size,depth_1,depth_2],stddev=tf.sqrt(1/batch_size))))\n",
        "    conv_2_biases  = tf.Variable(tf.zeros(depth_2))\n",
        "    #pool2 will be here\n",
        "    \n",
        "    \n",
        "    fc1_weights = (tf.Variable(tf.truncated_normal(\n",
        "                            [img_size//4*img_size//4*depth_2,fc_2_nodes],stddev=tf.sqrt(2/batch_size))))\n",
        "    \n",
        "    fc1_biases  = tf.Variable(tf.zeros(fc_2_nodes))\n",
        "    \n",
        "    \n",
        "    #another fc layer\n",
        "    \n",
        "    fc2_weights = tf.Variable(tf.truncated_normal([fc_2_nodes,fc_3_nodes],stddev=tf.sqrt(2/batch_size)))\n",
        "    fc2_biases = tf.Variable(tf.zeros(fc_3_nodes))\n",
        "    \n",
        "    #generation of logits\n",
        "    \n",
        "    fc3_weights = tf.Variable(tf.truncated_normal([fc_3_nodes,num_classes], stddev=tf.sqrt(1/batch_size)))\n",
        "    fc3_biases = tf.Variable(tf.zeros(num_classes))\n",
        "    \n",
        "    \n",
        "    def model(data):\n",
        "        \n",
        "        conv1= tf.nn.conv2d(data,conv_1_weights,[1,1,1,1], \"SAME\", name = \"convolution_1\")\n",
        "        pool1 = tf.layers.max_pooling2d(conv1,2,2,padding = 'same',data_format = 'channels_last')\n",
        "        \n",
        "        #applying relu\n",
        "        pool1_relu = tf.nn.relu(pool1)\n",
        "        \n",
        "        \"\"\"2nd convolution layer\"\"\"\n",
        "        conv2 = tf.nn.conv2d(pool1_relu,conv_2_weights,[1,1,1,1], \"SAME\", name = \"convolution_2\")\n",
        "        pool2 = tf.layers.max_pooling2d(conv2,2,2,padding = 'same')\n",
        "        pool_2_relu = tf.nn.relu(pool2)\n",
        "        \n",
        "        p2_shape = pool_2_relu.get_shape().as_list()\n",
        "        \n",
        "        fc1 = tf.reshape(pool_2_relu, [p2_shape[0],p2_shape[1]*p2_shape[2]*p2_shape[3] ])\n",
        "        \n",
        "        fc1_dr = tf.nn.dropout(fc1,keep_prob=0.8)\n",
        "        \n",
        "        fc2 = tf.nn.relu(tf.nn.xw_plus_b(fc1_dr,fc1_weights,fc1_biases))\n",
        "        \n",
        "        fc2_dr = tf.nn.dropout(fc2,keep_prob = 0.9)\n",
        "        \n",
        "        fc3 = tf.nn.relu(tf.nn.xw_plus_b(fc2_dr,fc2_weights,fc2_biases))\n",
        "        \n",
        "        logits = tf.nn.xw_plus_b(fc3,fc3_weights,fc3_biases)\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    \n",
        "    #computing loss\n",
        "    logits = model(X_train)\n",
        "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2\n",
        "                           (logits = logits,labels=y_train)))\n",
        "    \n",
        "    \n",
        "        \n",
        "    \n",
        "    #optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(0.002).minimize(loss)\n",
        "    \n",
        "    \n",
        "    #Making predictions\n",
        "    y_pred = tf.nn.softmax(logits)\n",
        "    y_valid= tf.nn.softmax(model(valid_dataset))\n",
        "    y_test = tf.nn.softmax(model(test_dataset))\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdqJFC5QDIwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's try interesting architectures for this problem.\n",
        "#1 - LeNet-5\n",
        "\n",
        "graph_lenet = tf.Graph()\n",
        "with graph_le"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jywVOYP9pDzp",
        "colab_type": "code",
        "outputId": "75605c2f-0317-4c18-b030-95000c4df514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 40\n",
        "\n",
        "with tf.Session(graph= graph_conv1) as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "    \n",
        "    for epoch in range(1,num_epochs+1):\n",
        "        start_batch = 0\n",
        "        \n",
        "        while start_batch<=train_size-batch_size:\n",
        "            \n",
        "            batch_data   = train_dataset[start_batch:min(start_batch+batch_size,train_size),:,:,:]\n",
        "            batch_labels = train_labels[start_batch:min(start_batch+batch_size,train_size),:]\n",
        "            \n",
        "            feed_dict = {X_train:batch_data,y_train:batch_labels}\n",
        "            \n",
        "            _,l,predictions = sess.run([optimizer,loss,y_pred],feed_dict=feed_dict)\n",
        "            \n",
        "            if(start_batch % 51200==0 and epoch%4==0):\n",
        "                print('Minibatch loss at epoch %d: %f' % (epoch, l))\n",
        "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
        "                \n",
        "            start_batch+=batch_size\n",
        "            \n",
        "        if(epoch%4==0):\n",
        "            print(\"\\nValidation accuracy after epoch {} is {} \\n\".format(epoch,accuracy(y_valid.eval(),valid_labels)))\n",
        "        \n",
        "    \n",
        "    print(\"\\nTest accuracy is {} \\n\".format(accuracy(y_test.eval(),test_labels)))\n",
        "                \n",
        "            \n",
        "            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-25b1d6537890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgraph_conv1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "606E101dpDzt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "lr = 0.0001 -> 83.64(76.21) with 75.8\n",
        "\n",
        "with 512 nodes in fc1\n",
        "lr = 0.001 ->87.91(81.58) with 83.6\n",
        "\n",
        "\n",
        "## Weight Initialization is extremely important\n",
        "Ok! An amazing result\n",
        "So Google guys in their original code had set stddev for conv weights to be 0.1 instead of 1,\n",
        "tried it amazingly results are very very different:\n",
        "\n",
        "lr = 0.001 -> 94.12(87.84!!) with mb_acc = 91.2 in 16 epochs\n",
        "lr = 0.001 -> 95.44(89.52!!!) with mb_acc = 93.4 in 32 epochs\n",
        "lr = 0.0015-> 95.34(89.89!!!) with mb_acc = 95.7 in 64 epochs with droput on last layer with keep_prob = 0.\n",
        "same netowork was able to reach 90% val accuracy at 68th epoch\n",
        "\n",
        "for fc_2_nodes = 256\n",
        "lr = 0.001 -> 92.31(86.24) with 89.1 in 8 epochs. This is very different from earlier results with same network structure, same lr where at this lr the accuracy started falling pretty quickly\n",
        "\n",
        "\n",
        "Now a research paper said that for relu layers stddev = sqrt(2/n), for others sqrt(1/n). I dont know whether they meant it for conv layers weights as well or not but anyhow I set stddev = sqrt(1/n) for all layers except relu and....\n",
        "\n",
        "By 4TH EPOCH ONLY Val. accuracy was at 91.19% !!!!(Yes by 4th epoch itself)\n",
        "final results after 80 epochs->\n",
        "\n",
        "97.01(92.12!!!!!!!) with 99.0\n",
        "\n",
        "With a modified network structure, bs = 4096, with two fully connected layers between conv and logits each with 512 nodes\n",
        "\n",
        "keep_prob for both layers kept at 0.9\n",
        "lr = 0.001 -> 96.94(92.25) with 94.8\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "noKFb2UovVFR",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_steps = 1001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print('Initialized')\n",
        "  for step in range(num_steps):\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 50 == 0):\n",
        "      print('Minibatch loss at step %d: %f' % (step, l))\n",
        "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
        "      print('Validation accuracy: %.1f%%' % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}